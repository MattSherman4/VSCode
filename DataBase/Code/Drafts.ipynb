{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import re\n",
    "from io import StringIO\n",
    "import time\n",
    "import warnings\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----- DRAFT ----- ###\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "draft = pd.DataFrame(columns=['Year', 'Rnd', 'Pick', 'Team', 'Player', 'Pos', 'Age', 'To', 'AP1', 'PB', 'St', 'wAV', 'DrAV', 'G', 'Pass_Cmp', 'Pass_Att', 'Pass_Yds', 'Pass_TD', 'Pass_Int',\n",
    "                    'Rush_Att', 'Rush_Yds', 'Rush_TD', 'Rec_Att', 'Rec_Yds', 'Rec_TD', 'Def_solo', 'Def_Int', 'Def_Sk', 'College'])\n",
    "\n",
    "for year in range(1936, datetime.now().year + 1):\n",
    "    if year % 9 == 0:\n",
    "        time.sleep(60)\n",
    "    # --- Collect Data --- #\n",
    "    s = 'https://www.pro-football-reference.com/years/%s/draft.htm' % str(year)\n",
    "    url = requests.get(s)\n",
    "    print(url)\n",
    "    parse = BeautifulSoup(url.text, \"html.parser\").prettify()                                               \n",
    "    start = parse.find(\"<table\")\n",
    "    end = parse.find(\"</table\")\n",
    "    temp = pd.read_html(parse[start:end+8])[0]\n",
    "\n",
    "    # --- Manipulate Data --- #\n",
    "    temp.columns = temp.columns.droplevel()\n",
    "    if year < 1994:\n",
    "        temp.insert(24, 'Def_solo', np.nan)\n",
    "    temp.insert(0, 'Year', year)\n",
    "    temp.columns = ['Year', 'Rnd', 'Pick', 'Team', 'Player', 'Pos', 'Age', 'To', 'AP1', 'PB', 'St', 'wAV', 'DrAV', 'G', 'Pass_Cmp', 'Pass_Att', 'Pass_Yds', 'Pass_TD', 'Pass_Int',\n",
    "                    'Rush_Att', 'Rush_Yds', 'Rush_TD', 'Rec_Att', 'Rec_Yds', 'Rec_TD', 'Def_solo', 'Def_Int', 'Def_Sk', 'College', 'DELETE']\n",
    "    temp.drop(['DELETE'], axis = 1, inplace = True)\n",
    "    temp = temp[temp['Player'] != 'Player']\n",
    "\n",
    "    # --- Combine Data --- #\n",
    "    draft = pd.concat([draft, temp])\n",
    "\n",
    "    print(year)\n",
    "\n",
    "draft.to_csv('C:\\\\Users\\\\pensh\\\\Desktop\\\\VSCode\\\\DataBase\\\\Data\\\\Draft\\\\Draft.csv')\n",
    "draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----- SUPPLEMENTAL DRAFT ----- ###\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Scrape Data --- #\n",
    "s = 'https://en.wikipedia.org/wiki/List_of_NFL_supplemental_draft_picks'\n",
    "url = requests.get(s)\n",
    "parse = BeautifulSoup(url.text, \"html.parser\").prettify()\n",
    "start = [m.start() for m in re.finditer('<table', parse)]\n",
    "end = [m.start() for m in re.finditer('</table', parse)]\n",
    "supplemental = pd.read_html(parse[start[3]:end[3]+8])[0]\n",
    "\n",
    "# --- Manipulate Data --- #\n",
    "supplemental['Player'] = supplemental['Player'].str.split(pat = ' †')\n",
    "supplemental['Player'] = supplemental['Player'].apply(lambda x: x[0])\n",
    "supplemental['Player'] = supplemental['Player'].str.split(pat = r'\\[\\d+\\]')\n",
    "supplemental['Player'] = supplemental['Player'].apply(lambda x: x[0])\n",
    "\n",
    "supplemental.to_csv('C:\\\\Users\\\\pensh\\\\Desktop\\\\VSCode\\\\DataBase\\\\Data\\\\Draft\\\\Supplemental.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----- Combine ----- ###\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "combine = pd.DataFrame(columns=['Year', 'Player', 'Pos', 'College', 'DELETE', 'Ht', 'Wt', '40yd', 'Vertical', 'Bench', 'Broad Jump', '3Cone', 'Shuttle', 'Drafted'])\n",
    "\n",
    "for year in range(2000, datetime.now().year + 1):\n",
    "    if year % 9 == 0:\n",
    "        time.sleep(60)\n",
    "    # --- Collect Data --- #\n",
    "    s = 'https://www.pro-football-reference.com/draft/%s-combine.htm' % str(year)\n",
    "    url = requests.get(s)\n",
    "    print(url)\n",
    "    parse = BeautifulSoup(url.text, \"html.parser\").prettify()                                               \n",
    "    start = parse.find(\"<table\")\n",
    "    end = parse.find(\"</table\")\n",
    "    temp = pd.read_html(parse[start:end+8])[0]\n",
    "\n",
    "    # --- Manipulate Data --- #\n",
    "    temp.insert(0, 'Year', year)\n",
    "    temp.columns = ['Year', 'Player', 'Pos', 'College', 'DELETE', 'Ht', 'Wt', '40yd', 'Vertical', 'Bench', 'Broad Jump', '3Cone', 'Shuttle', 'Drafted']\n",
    "    temp.drop(['DELETE'], axis = 1, inplace = True)\n",
    "    temp = temp[temp['Player'] != 'Player']\n",
    "\n",
    "    # --- Combine Data --- #\n",
    "    combine = pd.concat([combine, temp])\n",
    "\n",
    "    print(year)\n",
    "\n",
    "combine.to_csv('C:\\\\Users\\\\pensh\\\\Desktop\\\\VSCode\\\\DataBase\\\\Data\\\\Combine\\\\Combine.csv')\n",
    "combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----- Transactions ----- ###\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "transactions = pd.DataFrame(columns=['Year', 'Month', 'Type', 'From', 'To', 'Date', 'Player', 'Position', 'Transaction'])\n",
    "type = ['trades', 'signings', 'reserve-list', 'waivers', 'terminations', 'other']\n",
    "\n",
    "for t in type:\n",
    "    for year in range(1965, datetime.now().year + 1):\n",
    "        for month in range(1, 13, 1):\n",
    "            # --- Skip Future --- #\n",
    "            if (year == datetime.now().year) & (month > datetime.now().month):\n",
    "                continue\n",
    "\n",
    "            # --- Catch Empty Tables --- #\n",
    "            try:\n",
    "                # --- Collect Data --- #\n",
    "                s = 'https://www.nfl.com/transactions/league/%s/%s/%s' % (t, str(year), str(month))\n",
    "                url = requests.get(s)\n",
    "                print(url)\n",
    "                parse = BeautifulSoup(url.text, \"html.parser\").prettify()                                               \n",
    "                start = parse.find(\"<table\")\n",
    "                end = parse.find(\"</table\")\n",
    "                temp = pd.read_html(parse[start:end+8])[0]\n",
    "\n",
    "                # --- Manipulate Data --- #\n",
    "                temp.insert(0, 'Type', t)\n",
    "                temp.insert(0, 'Month', month)\n",
    "                temp.insert(0, 'Year', year)\n",
    "                temp.columns = ['Year', 'Month', 'Type', 'From', 'To', 'Date', 'Player', 'Position', 'Transaction']\n",
    "\n",
    "                # --- Combine Data --- #\n",
    "                transactions = pd.concat([transactions, temp])\n",
    "            except:\n",
    "                print('No Data for: ' + t + ', ' + str(year) + ', ' + str(month))\n",
    "\n",
    "\n",
    "            print(t + ', ' + str(year) + ', ' + str(month))\n",
    "\n",
    "transactions.reset_index(drop = True, inplace = True)\n",
    "transactions.to_csv('C:\\\\Users\\\\pensh\\\\Desktop\\\\VSCode\\\\DataBase\\\\Data\\\\Transactions\\\\Transactions.csv')\n",
    "transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ----- Transactions1 ----- ###\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "Transactions1 = pd.DataFrame(columns=['Date', 'Team', 'Acquired', 'Relinquished', 'Notes'])\n",
    "team = [#'49ers', 'Bears', 'Bengals', 'Bills', 'Broncos', 'Browns', 'Buccaneers', 'Cardinals', 'Chargers', 'Chiefs', 'Colts', 'Commanders', 'Cowboys', 'Dolphins', 'Eagles', 'Falcons', 'Giants', 'Jaguars',\n",
    "    # 'Jets', 'Lions', 'Packers', 'Panthers', 'Patriots', 'Raiders',\n",
    "     'Rams', \n",
    "     'Ravens', 'Saints', 'Seahawks', 'Steelers', 'Texans', 'Titans', 'Vikings']\n",
    "\n",
    "for t in team:\n",
    "    for r in range(0, 16111, 1):\n",
    "        page = r * 25\n",
    "        # --- Collect Data --- #\n",
    "        s = 'https://www.prosportstransactions.com/football/Search/SearchResults.php?Player=&Team=%s&BeginDate=&EndDate=&PlayerMovementChkBx=yes&ILChkBx=yes&NBADLChkBx=yes&InjuriesChkBx=yes&PersonalChkBx=yes&DisciplinaryChkBx=yes&LegalChkBx=yes&submit=Search&start=%s' % (t, str(page))\n",
    "        url = requests.get(s)\n",
    "        print(url)\n",
    "        parse = BeautifulSoup(url.text, \"html.parser\").prettify()                                               \n",
    "        start = parse.find(\"<table\")\n",
    "        end = parse.find(\"</table\")\n",
    "        temp = pd.read_html(parse[start:end+8])[0]\n",
    "\n",
    "        # --- Manipulate Data --- #\n",
    "        temp.columns = ['Date', 'Team', 'Acquired', 'Relinquished', 'Notes']\n",
    "        temp = temp[temp['Date'] != 'Date']\n",
    "\n",
    "        # --- Break Check --- #\n",
    "        if len(temp.index) == 0:\n",
    "            break\n",
    "\n",
    "        # --- Combine Data --- #\n",
    "        Transactions1 = pd.concat([Transactions1, temp])\n",
    "\n",
    "        print('Team: ' + t + ', page ' + str(r))\n",
    "\n",
    "    endLink = 'C:\\\\Users\\\\pensh\\\\Desktop\\\\VSCode\\\\DataBase\\\\Data\\\\Transactions\\\\%s.csv' % t\n",
    "    Transactions1.to_csv(endLink)\n",
    "Transactions1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Collect Data --- #\n",
    "page = 0\n",
    "s = 'https://www.prosportstransactions.com/football/Search/SearchResults.php?Player=&Team=49ers&BeginDate=&EndDate=&PlayerMovementChkBx=yes&ILChkBx=yes&NBADLChkBx=yes&InjuriesChkBx=yes&PersonalChkBx=yes&DisciplinaryChkBx=yes&LegalChkBx=yes&submit=Search&start=%s' % str(2500)\n",
    "url = requests.get(s)\n",
    "print(url)\n",
    "parse = BeautifulSoup(url.text, \"html.parser\").prettify()                                               \n",
    "start = parse.find(\"<table\")\n",
    "end = parse.find(\"</table\")\n",
    "temp = pd.read_html(parse[start:end+8])[0]\n",
    "\n",
    "# --- Manipulate Data --- #\n",
    "# temp.insert(0, 'Year', year)\n",
    "temp.columns = ['Date', 'Team', 'Acquired', 'Relinquished', 'Notes']\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAFC', 'NFL', 'AFL', 'USFL'], dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### --- Clean Data --- ###\n",
    "\n",
    "team = ['49ers', 'Bears', 'Bengals', 'Bills', 'Broncos', 'Browns', 'Buccaneers', 'Cardinals', 'Chargers', 'Chiefs', 'Colts', 'Commanders', 'Cowboys', 'Dolphins', 'Eagles', 'Falcons', 'Giants', 'Jaguars', 'Jets', 'Lions', 'Packers', 'Panthers', 'Patriots', 'Raiders', 'Rams', 'Ravens', 'Saints', 'Seahawks', 'Steelers', 'Texans', 'Titans', 'Vikings']\n",
    "df = pd.DataFrame()\n",
    "for t in team:\n",
    "    temp = pd.read_csv('C:\\\\Users\\\\pensh\\\\Desktop\\\\VSCode\\\\DataBase\\\\Data\\\\Transactions\\\\%s.csv' % t)\n",
    "    temp.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "    temp['Acquired'] = temp['Acquired'].str.replace('• ', '')\n",
    "    temp['Relinquished'] = temp['Relinquished'].str.replace('• ', '')\n",
    "    df = pd.concat([df, temp])\n",
    "\n",
    "# df[df['Acquired'].str.contains(\"Le'Veon Bell\") == True]\n",
    "\n",
    "conditions = [df['Team'].str.contains('49ers'),\n",
    "              df['Team'] == 'Bears',\n",
    "              df['Team'].str.contains('Bengals'),\n",
    "              df['Team'].str.contains('Bills'),\n",
    "              df['Team'].str.contains('Broncos'),\n",
    "              df['Team'].str.contains('Browns'),\n",
    "              df['Team'] == 'Buccaneers',\n",
    "              df['Team'] == 'Cardinals',\n",
    "              df['Team'].str.contains('Chargers'),\n",
    "              (df['Team'].str.contains('Chiefs')) | (df['Team'] == 'Texans (AFL)'),\n",
    "              df['Team'] == 'Colts',\n",
    "              (df['Team'] == 'Commanders') | (df['Team'] == 'Washington') | (df['Team'] == 'Redskins'),\n",
    "              df['Team'] == 'Cowboys',\n",
    "              df['Team'].str.contains('Dolphins'),\n",
    "              df['Team'] == 'Eagles',\n",
    "              df['Team'] == 'Falcons',\n",
    "              df['Team'] == 'Giants',\n",
    "              df['Team'] == 'Jaguars',\n",
    "              (df['Team'].str.contains('Jets')) | (df['Team'] == 'Titans (AFL)'),\n",
    "              df['Team'] == 'Lions',\n",
    "              (df['Team'] == 'Minneapolis (AFL)') | (df['Team'] == 'Vikings'),\n",
    "              (df['Team'].str.contains('Oilers')) | (df['Team'] == 'Titans'),\n",
    "              df['Team'] == 'Packers',\n",
    "              df['Team'] == 'Panthers',\n",
    "              df['Team'].str.contains('Patriots'),\n",
    "              (df['Team'] == 'Pirates') | (df['Team'] == 'Steelers'),\n",
    "              df['Team'].str.contains('Raiders'),\n",
    "              df['Team'] == 'Rams',\n",
    "              df['Team'] == 'Ravens',\n",
    "              df['Team'] == 'Saints',\n",
    "              df['Team'] == 'Seahawks',\n",
    "              df['Team'] == 'Texans',\n",
    "              df['Team'] == 'Wranglers (USFL)',\n",
    "              df['Team'] == 'Gamblers (USFL)',\n",
    "              df['Team'] == 'Breakers (USFL)',\n",
    "              df['Team'] == 'Bandits (USFL)',]\n",
    "franchises = ['49ers', 'Bears', 'Bengals', 'Bills', 'Broncos', 'Browns', 'Buccaneers', 'Cardinals', 'Chargers', 'Chiefs', 'Colts', 'Commanders', 'Cowboys', 'Dolphins', 'Eagles', 'Falcons', 'Giants', 'Jaguars', 'Jets', 'Lions', 'Vikings', 'Titans', 'Packers', 'Panthers', 'Patriots', 'Steelers', 'Raiders', 'Rams', 'Ravens', 'Saints', 'Seahawks', 'Texans', 'Wranglers', 'Gamblers', 'Breakers', 'Bandits']\n",
    "\n",
    "df['Franchise'] = np.select(conditions, franchises, \"UNKNOWN\")\n",
    "\n",
    "def league(t):\n",
    "    if '(AFL)' in t:\n",
    "        return 'AFL'\n",
    "    elif '(AAFC)' in t:\n",
    "        return 'AAFC'\n",
    "    elif '(USFL)' in t:\n",
    "        return 'USFL'\n",
    "    elif '(' not in t:\n",
    "        return 'NFL'\n",
    "    else:\n",
    "        return 'UNKNOWN'\n",
    "\n",
    "df['League'] = df['Team'].apply(league)\n",
    "df['League'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "team = ['49ers', 'Bears', 'Bengals', 'Bills', 'Broncos', 'Browns', 'Buccaneers', 'Cardinals', 'Chargers', 'Chiefs', 'Colts', 'Commanders', 'Cowboys', 'Dolphins', 'Eagles', 'Falcons', 'Giants', 'Jaguars', 'Jets', 'Lions', 'Packers', 'Panthers', 'Patriots', 'Raiders', 'Rams', 'Ravens', 'Saints', 'Seahawks', 'Steelers', 'Texans', 'Titans', 'Vikings']\n",
    "df = pd.DataFrame()\n",
    "for t in team:\n",
    "    temp = pd.read_csv('C:\\\\Users\\\\pensh\\\\Desktop\\\\VSCode\\\\DataBase\\\\Data\\\\Transactions\\\\%s.csv' % t)\n",
    "    temp.drop('Unnamed: 0', axis = 1, inplace = True)\n",
    "    temp['Acquired'] = temp['Acquired'].str.replace('• ', '')\n",
    "    temp['Relinquished'] = temp['Relinquished'].str.replace('• ', '')\n",
    "    df = pd.concat([df, temp])\n",
    "\n",
    "df.drop_duplicates(inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "df.loc[((df['Date'] == '1899-12-30') & (df['Acquired'] == 'Ahmad Merritt')), 'Date'] = '2000-04-16'\n",
    "df.loc[((df['Date'] == '1899-12-30') & (df['Acquired'] == 'Chris Lammons')), 'Date'] = '2023-01-24'\n",
    "df = df[df['Date'].str.contains('1899') == False]\n",
    "\n",
    "# df[df['Team'] == 'Steelers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'cannot use a single bool to index into setitem'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mType\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNotes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSigned\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mType\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSigned\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# temp = df['Notes'].str.split(' ').tolist()\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# l = [print(item) for item in temp]\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexing.py:849\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    848\u001b[0m iloc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miloc\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39miloc\n\u001b[1;32m--> 849\u001b[0m \u001b[43miloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_with_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexing.py:1737\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1733\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(indexer):\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1735\u001b[0m         \u001b[38;5;66;03m# reindex the axis to the new value\u001b[39;00m\n\u001b[0;32m   1736\u001b[0m         \u001b[38;5;66;03m# and set inplace\u001b[39;00m\n\u001b[1;32m-> 1737\u001b[0m         key, _ \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_missing_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1739\u001b[0m         \u001b[38;5;66;03m# if this is the items axes, then take the main missing\u001b[39;00m\n\u001b[0;32m   1740\u001b[0m         \u001b[38;5;66;03m# path first\u001b[39;00m\n\u001b[0;32m   1741\u001b[0m         \u001b[38;5;66;03m# this correctly sets the dtype and avoids cache issues\u001b[39;00m\n\u001b[0;32m   1742\u001b[0m         \u001b[38;5;66;03m# essentially this separates out the block that is needed\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m         \u001b[38;5;66;03m# to possibly be modified\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m==\u001b[39m info_axis:\n\u001b[0;32m   1745\u001b[0m             \u001b[38;5;66;03m# add the new item, and set the value\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m             \u001b[38;5;66;03m# must have all defined axes if we have a scalar\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m             \u001b[38;5;66;03m# or a list-like on the non-info axes if we have a\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m             \u001b[38;5;66;03m# list-like\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pandas\\core\\indexing.py:2538\u001b[0m, in \u001b[0;36mconvert_missing_indexer\u001b[1;34m(indexer)\u001b[0m\n\u001b[0;32m   2535\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2537\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indexer, \u001b[38;5;28mbool\u001b[39m):\n\u001b[1;32m-> 2538\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot use a single bool to index into setitem\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m indexer, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexer, \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'cannot use a single bool to index into setitem'"
     ]
    }
   ],
   "source": [
    "df['Type'] = np.nan\n",
    "df.loc[df['Notes'].str.split(' ')[0] == 'Signed', 'Type'] = 'Signed'\n",
    "# temp = df['Notes'].str.split(' ').tolist()\n",
    "# l = [print(item) for item in temp]\n",
    "\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
